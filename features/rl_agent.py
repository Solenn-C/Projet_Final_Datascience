import pandas as pd
import numpy as np
import os
from stable_baselines3 import PPO
from rl_env import TradingEnv

# 1. CrÃ©ation du dossier modÃ¨le
if not os.path.exists('models'):
    os.makedirs('models')

# 2. Chargement et Nettoyage des donnÃ©es
print("ğŸ“Š Chargement des donnÃ©es...")
df_raw = pd.read_csv('data/GBPUSD_M15_CLEANED.csv')
features = ['Open', 'High', 'Low', 'Close']
df = df_raw[features].copy()

# Suppression des valeurs aberrantes et remplissage des vides
df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill().fillna(0.0)

# Normalisation Min-Max robuste
df_norm = (df - df.min()) / (df.max() - df.min() + 1e-5)

print(f"âœ… DonnÃ©es prÃªtes ({len(df_norm)} lignes).")

# 3. Configuration de l'environnement et du modÃ¨le
env = TradingEnv(df_norm)

model = PPO(
    "MlpPolicy",
    env,
    verbose=1,
    learning_rate=0.0001,
    gamma=0.99,
    ent_coef=0.02, # Un peu de curiositÃ© pour Ã©viter le plat
    seed=42
)

# 4. EntraÃ®nement
print("ğŸš€ Lancement de l'entraÃ®nement...")
model.learn(total_timesteps=100000)

# 5. Sauvegarde
model.save("models/ppo_gbpusd_agent")
print("âœ… ModÃ¨le sauvegardÃ© : models/ppo_gbpusd_agent.zip")